{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Financial Text Classification Using FinBERT With Pytorch","metadata":{"_uuid":"5ab6ce3b-f5f7-468d-84fe-ad74d7d37367","_cell_guid":"3c6b6c1e-77e8-491e-9707-9d6e6f094957","trusted":true}},{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\"\"\"\nSklearn Libraries\n\"\"\"\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"\nTransformer Libraries\n\"\"\"\nfrom transformers import BertTokenizer,  AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\n\"\"\"\nPytorch Libraries\n\"\"\"\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset","metadata":{"_uuid":"33a15389-83ac-4e50-a781-57d017f1a19c","_cell_guid":"8c6fe61b-b916-4c91-a27d-49fe4c23f17b","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''#def show_headline_distribution(sequence_lengths, figsize = (15,8)):\n    \n    \n    # Get the percentage of reviews with length > 512\n    #len_512_plus = [rev_len for rev_len in sequence_lengths if rev_len > 512]\n    #percent = (len(len_512_plus)/len(sequence_lengths))*100\n    \n    print(\"Maximum Sequence Length is {}\".format(max(sequence_lengths)))\n    \n    # Configure the plot size\n    plt.figure(figsize = figsize)\n\n    sns.set(style='darkgrid')\n    \n    # Increase information on the figure\n    sns.set(font_scale=1.3)\n    \n    # Plot the result\n    sns.distplot(sequence_lengths, kde = False, rug = False)\n    plt.title('Headlines Lengths Distribution')\n    plt.xlabel('Headlines Length')\n    plt.ylabel('Number of Headlines')'''","metadata":{"_uuid":"3cd0b66b-484e-4bdd-a8ac-dd116f9c5787","_cell_guid":"7ba8bff2-9449-4f84-b28b-21f16650eda4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"financial_data = pd.read_csv(\"https://raw.githubusercontent.com/vrunm/nlp-datasets/main/all-data.csv\", \n                             encoding='latin-1', \n                             names=['sentiment', 'NewsHeadline'])","metadata":{"_uuid":"b30c959f-a961-41b8-b2dc-5667e9ef44ca","_cell_guid":"9def1c76-1711-4826-8e5b-54ef2e120355","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"financial_data.head()","metadata":{"_uuid":"d2edd711-25c3-45eb-9a7a-de9aa48d0001","_cell_guid":"dc23c584-bf3a-43af-a43b-f3430c6e6dfb","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"financial_data.shape","metadata":{"_uuid":"5f356016-61ad-4079-b9ea-5d828e158e60","_cell_guid":"c7bfd0a6-a461-47a1-9834-82b8a988c2f0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"financial_data.sentiment.value_counts()","metadata":{"_uuid":"85dcdbdd-8d8d-46f8-86a3-78c97393d17a","_cell_guid":"bcae579b-1a28-4e05-b76c-bec8dc69bb29","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configure the plot size\nplt.figure(figsize = (15,8))    \n# Increase information on the figure\nsns.set(font_scale=1.3)\nsns.countplot(x='sentiment', data = financial_data)\nplt.title('News Sentiment Distribution')\nplt.xlabel('News Polarity')\nplt.ylabel('Number of News')","metadata":{"_uuid":"721c343a-ee2a-4d7f-b8eb-3726666ff8c1","_cell_guid":"041b7dc6-b8b0-4337-866b-88fb2b159063","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_headlines_len(df):\n    \n    headlines_sequence_lengths = []\n    \n    print(\"Encoding in progress...\")\n    for headline in tqdm(df.NewsHeadline):\n        encoded_headline = finbert_tokenizer.encode(headline, \n                                         add_special_tokens = True)\n        \n        # record the length of the encoded review\n        headlines_sequence_lengths.append(len(encoded_headline))\n    print(\"End of Task.\")\n    \n    return headlines_sequence_lengths","metadata":{"_uuid":"a9154daa-4983-4c0a-8a2d-b726bd82daac","_cell_guid":"c5894b90-a1c3-4b73-8bee-672a159928f9","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{"_uuid":"4698d08b-ebf7-4174-aa1a-d2b64e2331c9","_cell_guid":"c8daa7b7-7b57-4405-a19f-1ef6988d8bbd","trusted":true}},{"cell_type":"code","source":"#Function to encode the sentiment values\n#The sentiments positive negative and neutral are mapped to 0,1,2\ndef encode_sentiments_values(df):\n    \n    possible_sentiments = df.sentiment.unique()\n    sentiment_dict = {}\n    \n    for index, possible_sentiment in enumerate(possible_sentiments):\n        sentiment_dict[possible_sentiment] = index\n    \n    # Encode all the sentiment values\n    df['label'] = df.sentiment.replace(sentiment_dict)\n    \n    return df, sentiment_dict","metadata":{"_uuid":"c6d3cd5f-4bc3-4b64-9635-93d12c1e4943","_cell_guid":"42bac800-fd23-4bd4-ba21-07e4319206a8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the sentiment column\nfinancial_data, sentiment_dict = encode_sentiments_values(financial_data)\nfinancial_data.head()","metadata":{"_uuid":"c74bc002-25a4-469c-a67c-d61a00daa5a5","_cell_guid":"83070786-9d12-40d2-842e-f0f2ab9ed363","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create training and validation data\nX_train, X_val, y_train, y_val = train_test_split(financial_data.index.values, \n                                                  financial_data.label.values, \n                                                  test_size = 0.15, \n                                                  random_state = 2022, \n                                                  stratify = financial_data.label.values)","metadata":{"_uuid":"70081665-8a75-4c58-89fa-7a1e6144f9f4","_cell_guid":"23302b38-d6c5-4b66-af29-314d3931a21a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"_uuid":"63e72275-5758-4127-97ac-1dbc6e0426c3","_cell_guid":"93768b35-76d8-4a55-a495-52c605cc0d88","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the data type columns\nfinancial_data.loc[X_train, 'data_type'] = 'train'\nfinancial_data.loc[X_val, 'data_type'] = 'val'\n\n# Visualize the number of sentiment occurence on each type of data\nfinancial_data.groupby(['sentiment', 'label', 'data_type']).count()","metadata":{"_uuid":"cb16a08c-dfed-49fd-87c6-2e4037e890f7","_cell_guid":"7e08b872-1d98-4f60-8d1d-fb2c9cd75762","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the FinBERT Tokenizer\nfinbert_tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\",do_lower_case=True)\n\n# Encode the Training and Validation Data\n#Hugging Face tokenizer\n#batch_text_or_text_pairs (List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], \n#List[str]]], and for not-fast tokenizers, also List[List[int]], List[Tuple[List[int], List[int]]]) — Batch of sequences or pair of sequences to be encoded. This can be a list of string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see details in encode_plus).\n#add_special_tokens (bool, optional, defaults to True) — Whether or not to encode the sequences with the special tokens relative to their model.\n#padding (bool, str or PaddingStrategy, optional, defaults to False) — Activates and controls padding. Accepts the following values:\n#True or 'longest': Pad to the longest sequence in the batch (or no padding if only a single sequence if provided).\n#'max_length': Pad to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided.\n#False or 'do_not_pad' (default): No padding (i.e., can output a batch with sequences of different lengths).\n#return_attention_mask (bool, optional) — Whether to return the attention mask. \n#If left to the default, will return the attention mask according to the specific tokenizer’s default, defined by the return_outputs attribute.\n#return_tensors (str or TensorType, optional) — If set, will return tensors instead of list of python integers. Acceptable values are:\n#'tf': Return TensorFlow tf.constant objects.\n#'pt': Return PyTorch torch.Tensor objects.\n#'np': Return Numpy np.ndarray objects.\n\n\n\nencoded_data_train = finbert_tokenizer.batch_encode_plus(\n    financial_data[financial_data.data_type=='train'].NewsHeadline.values, \n    return_tensors='pt',\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=150 )\n\nencoded_data_val = finbert_tokenizer.batch_encode_plus(\n    financial_data[financial_data.data_type=='val'].NewsHeadline.values, \n    return_tensors='pt',\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=150 )\n\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(financial_data[financial_data.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nsentiments_val = torch.tensor(financial_data[financial_data.data_type=='val'].label.values)\n\n#Creating the train dataset \n#class:torch.utils.data.TensorDataset(*tensors)[source]\n#Dataset wrapping tensors.\n#Each sample will be retrieved by indexing tensors along the first dimension.\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n\n#Creating the test dataset \ndataset_val = TensorDataset(input_ids_val, attention_masks_val, sentiments_val)","metadata":{"_uuid":"9ac3c85d-c2f2-4a19-a9d9-063239e6b092","_cell_guid":"675dc171-da55-4af8-bb72-0902c0840ade","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining the model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\",\n                                                          num_labels=len(sentiment_dict),\n                                                          output_attentions=False,\n                                                          output_hidden_states=False)","metadata":{"_uuid":"ed374c8a-5a78-44a4-8caa-e5cb8b86e943","_cell_guid":"5989874e-9b9d-4a52-9bcd-5068d41285cf","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#headlines_sequence_lengths = get_headlines_len(financial_data)","metadata":{"_uuid":"5fbb145e-b604-4719-ae33-d35aadeb146a","_cell_guid":"d8d86a79-9c73-4029-8c4b-96f8d86f0866","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n# Show the reviews distribution \nThe overall implementation of this function is in my notebook at end of the article\n'''\n#show_headline_distribution(headlines_sequence_lengths)","metadata":{"_uuid":"a1ad159d-c55b-44e2-ab58-2f5203f31b2c","_cell_guid":"7894295c-eecf-45b4-ab45-0eeab95b2e92","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=batch_size)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=batch_size)","metadata":{"_uuid":"c46826d8-47fc-4b9b-85a4-9e4f47efd61a","_cell_guid":"5acebaef-9813-44a2-b9dc-23e2544a8548","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\noptimizer1 = torch.optim.AdamW(model.parameters(),lr=5e-5,eps=1e-8)\noptimizer2 = torch.optim.SGD(model.parameters(),lr=0.01)\noptimizer3 = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.001)\noptimizer4 = torch.optim.RMSprop(model.parameters(),lr=0.01, alpha=0.99, eps=1e-08, momentum=0.01)\noptimizer5 = torch.optim.Adagrad(model.parameters(),lr=0.01, lr_decay=0, weight_decay=0)\n\nepochs = 3\n\nscheduler = get_linear_schedule_with_warmup(optimizer2, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)","metadata":{"_uuid":"216724d8-199f-41d0-868a-98f3e1901f7a","_cell_guid":"a1a9b920-16ae-463c-9c88-4017554452f5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","metadata":{"_uuid":"06f83219-5c75-4d8b-9323-087733b1ff28","_cell_guid":"f08ebfd7-f90e-4b20-baf1-ca19ebc94cd2","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_val = 2022\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n\ndef evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], [],\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)    \n    return loss_val_avg, predictions, true_vals\n\n\nfor epoch in tqdm(range(1, epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer3.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_finBERT_epoch_{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    loss_train_avg = loss_train_total/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    \n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')\n    #print(train_acc = torch.sum(y_pred == true_vals))","metadata":{"_uuid":"c99a47e2-1e3e-4716-8c93-b4343773cc20","_cell_guid":"b4a3bea8-1639-454e-ba73-135064521841","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in sentiment_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    acc = []\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n        acc.append((len(y_preds[y_preds==label])) / (len(y_true)))\n    print(\"Model Accuracy: \", np.mean(acc)*100)","metadata":{"_uuid":"1927809a-14c0-4a4e-9727-9f1f0d3f0bb9","_cell_guid":"3ab107dd-71de-48c9-bf39-b33273ebae0a","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy(preds,labels):  \n    label_dict_inverse = {v: k for k, v in sentiment_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    acc = []\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(y_preds)\n        print(y_true)\n        #print(f'Accuracy: {(y_preds[y_preds==label])}/{len(y_preds)}\\n')","metadata":{"_uuid":"3a4e1ee6-7e68-4038-a500-4042db3a2650","_cell_guid":"86b301d3-9702-4488-9284-487e196ba926","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model & Make Predictions\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\",\n                                                          num_labels=len(sentiment_dict),\n                                                          output_attentions=False,\n                                                          output_hidden_states=False)\n\nmodel.to(device)\nmodel.load_state_dict(torch.load('finetuned_finBERT_epoch_1.model', \n                                 map_location=torch.device('cpu')))\n\n_, predictions, true_vals = evaluate(dataloader_validation)\n\naccuracy_per_class(predictions, true_vals)\n#accuracy(predictions, true_vals)","metadata":{"_uuid":"d7b5413a-e2e7-48b9-a20c-2f61ba291b83","_cell_guid":"5fb43b31-73df-4bc1-939d-850ca8a7ef6e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}